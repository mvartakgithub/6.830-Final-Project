\documentclass[12pt]{article}
\setcounter{secnumdepth}{2} 
\title{PubCrawl: A Publication Search Engine}
\author{Frank Li (frankli@mit.edu) \\
Abhishek Sarkar (aksarkar@mit.edu)\\
Manasi Vartak (mvartak@mit.edu)}
\date{May 16, 2012}

\usepackage{graphicx}

\newcommand{\citeseer}{CiteSeer$^\chi$}

\begin{document}
\maketitle

\begin{abstract}
Searching for documents ranked by relevance is a well-studied problem for web
pages. In particular, there are frameworks for estimating relevance as a
function of the content of the document and algorithms for estimating relevance
as a function of incoming links. However, further specialization of these
techniques is required to search a corpus of academic papers. Academic papers
are highly structured, lending additional weight to matches which appear in
important sections such as the title or abstract. Moreover, citations have
contextual information which changes the weight which should be contributed to
cited papers. Here we describe PubCrawl, a search engine built for the ACM
Digital Library. The system allows users to weight the relative importance of
matches in various sections of the documents. We also implement a version of
PageRank over the dataset. We evaluate the system in user studies.
\end{abstract}

\section{Introduction}

A primary source of information for computer science researchers are published
papers from conference proceedings and journals. As the number of papers and
venues increases, the ability to quickly find papers relevant to a given topic
becomes more important. In particular, researchers want search engines which
return important papers.

There are several existing search engines for academic papers, most prominently
Google Scholar, \citeseer, and Microsoft Academic Search. Google Scholar uses a
proprietary algorithm which combines several approaches. However, researchers
have partially reverse engineered it \cite{beel09a, beel09b}. In particular,
the algorithm is biased towards highly cited papers, which should highly rank
important papers in the standard literature. However, the bias
will also make the algorithm miss more recent influential papers. Researchers
also uncovered several weaknesses in Google's automatic processing of documents
which could allow authors to game search rankings \cite{beel10}.

\citeseer\ is primarily an automatic citation indexing system \cite{giles98,
  lawrence99, li06}. The system crawls the web and processes documents which
appear to be academic papers. This approach produces a large corpus of noisy
data, which hurts the performance of its ranking algorithm. Also,
\citeseer\ does not make use of the citation data it processes in its ranking
algorithm.

Microsoft Academic Search is a relatively new search engine designed to compete
with Google Scholar. It too uses a proprietary algorithm which combines several
features, and shows the same weakness to ranking optimization as Scholar.

Here, we present PubCrawl, a system built to test ranking algorithms for
searching academic papers. The system searches over the complete ACM Digital
Library. We use Apache Lucene \cite{lucene} to index documents and estimate relevance and an adapted PageRank \cite{page99} to estimate importance. The system provides a web interface using the Django web framework backed by a Postgres database. Users can input queries and relative weights for matches in various sections of the document as well as the PageRank.

\section{ACM Data}
%Information about ACM data

The complete ACM Digital Library was obtained from the Association for Computing Machinery. It consists of 228,300 papers, primarily those published in ACM-affiliated venues. The data contains both paper text and meta-data in XML format.

One important meta-data recorded is a paper's citations. This allows the data to form a citation graph, where papers are nodes and an edge from a paper A to a paper B indicates A cites B. The citation graph of the ACM data set has two interesting features that become relevant to our investigation: cycles and components.

In citation cycles, a paper's chain of citations returns to the paper itself. A number of these cycles exist in the data set, including self-citations and cycles of several papers. Typically, a self-citation occurs when a paper cites a different document with the same title and author but no publication information, such as a web page. Multi-paper cycles tend to be published around the same time. Often the cycle's papers are published in the same venue, making it impossible to determine a chronological ordering of the cycle.

The citation graph of the ACM data is not completely connected. The graph is composed of a number of components, where papers
in the same component are either both part of the same citation chain or share
a common ancestor or descendant in their respective chain. This largely arises because many citation links are missing since the ACM data is limited in coverage to ACM papers. The ACM data
exhibits 45,571 total components, with one large component of 176,285 papers,
and a large number of smaller components ranging in size from one to ten
papers, as displayed in Figure \ref{comp-size}. 

\begin{figure}[t]
\centering \includegraphics[width=5in]{images/component-graph}
	\caption{Sizes of components in the ACM data set and the number of each
      size's occurrence.}
	\label{comp-size}
\end{figure}



\section{PubCrawl Design}

PubCrawl is designed to rank papers based on relevance and importance. To
measure relevance, we use Apache Lucene, a high-performance text search engine
library for Java \cite{lucene}. Lucene is used to provide text indexing, returning the papers
most textually relevant to a query. To measure importance, we use a variation
of the classic PageRank algorithm \cite{page99} on the citation graph of papers. The weighting between the text indexing and PageRank score is an input the users
can provide, as discussed in \ref{sec:implementation}. Currently, the ACM
dataset has been gathered and applied using PubCrawl. Through the design of the
system, we discuss the effect of a number of its features.

\subsection{Document scores}

We use Apache Lucene \cite{lucene} to index documents and provide scores based
on the content of documents. We build the usual inverted index over the corpus,
filtering out stopwords. The structure of the documents is preserved in the XML
corpus, so we can index titles, subtitles, abstracts, and full text as separate
fields. We also include various metadata to simplify the implementation of the
front-end.

Lucene uses the Boolean model \cite{lancaster73} to filter documents and the
Vector Space Model \cite{salton75} to score documents. Documents $d$ and
queries $q$ are vectors $V(\cdot)$ with each term a dimension and coordinates
equal to tf*idf values. Then, the relevance of a document to a query is related
to the angle between them (the cosine similarity score). Lucene generalizes the
cosine similarity score in several ways. First, documents are allowed to have
multiple and duplicate fields. Second, Lucene allows index-time boosting of
documents and fields as well as query-time boosting of query terms. Finally,
Lucene does not require every term to appear in hits, instead using a
coordination factor to score documents which contain more terms higher. These
features can be exploited to improve the quality of search results.

\subsection{Adapted PageRank}
\label{sec:pagerank}

The relationship between academic papers draws a strong analogy with that
between web pages. Just as pages on the web embed links to other pages, papers
cite other papers. This forms a directed graph of paper nodes and citation
edges, where an edge from one paper $A$ to another paper $B$ indicates that $A$
cited $B$.

To exploit the similarity between the two environments, we use a variation of
the PageRank algorithm \cite{page99}. PageRank is an algorithm used by the Google search engine that provides a weighted score for web pages based on the number of
external pages that cite a particular web page, and the PageRank of those
external pages. We use a similar framework for academic papers, where a paper's
score is based on the number of other papers citing it, and the score of those
papers. The most simple form of this captured by the following equation:\\
\begin{equation}
\label{eqn:basic}
S(p)= \Sigma_{c \in C} \frac{S(c)}{D(c)}
\end{equation}
where $p$ is the paper being scored, $C$ is the set of all papers that cite
$p$, $S$ is the scoring function, and $D$ is the degree function (indicating
the number of papers a certain paper cites).

The score of all nodes can be calculated in an iterative manner. All nodes can
be initialized to a default score, such as 1. Then, in each round, Equation
\ref{eqn:basic} can be applied to all nodes. This process is repeated until
node scores converge. However, this scoring function is further complicated by
a couple of features of academic paper relations, specifically those seen in the ACM data set: cycles and components.

Citation cycles provide one source of complication. In general, these cycles act as a score sink, shifting the score of external nodes into the cycle round after round. Inside the cycle, the score rotates among nodes, preventing convergence. To reduce the impact of these cycles, a score source is provided for node each round, as also done in 
. To provide convergence, the total sum of all node scores must remain constant, so node scores must be normalized per round. This results in an updated scoring function of:\\
\begin{equation}
\label{eqn:cycle}
S(p)= n(\Sigma_{c \in C} \frac{S(c)}{D(c)}+E(p))
\end{equation}
where $E$ is a source of score, $n$ is the normalization factor, and all other
variables remain as defined.

Components in the citation graph are the other source of complication. Small components are papers
that don't cite many or any other papers in the data set and aren't heavily
cited. This implies that nodes in small components should not be highly ranked. To adjust for this, nodes are categorized into the components they belong in.
Instead of initializing node scores to be a constant value such as 1, they are
instead initialized to a score linear to the size of the component containing
the nodes. Intuitively, this means that papers in larger components will
receive higher scores, as desired.

A final variation on the basic scoring function is the incorporation of a
damping factor $d$. A user following a citation chain will eventually stop,
possibly at the end of the chain. Intuitively, the farther down a chain a user
traverses, the less likely the papers will relevant and important to the
original query, and the more likely the user will stop. The damping factor
models this behavior by reducing the change produced by the scoring function
as the number of rounds increases. Adapted from PageRank, this results in the
final version of our scoring function:\\
\begin{equation}
\label{eqn:damping}
S(p)= n(\frac{1-d}{N}+d(\Sigma_{c \in C} \frac{S(c)}{D(c)}+E(p)))
\end{equation}
where $d$ is the damping factor, $N$ is the total number of papers/nodes in the
network, and all other variables remain as defined.

\section{Implementation}
\label{sec:implementation}

%\subsection{Overview}
PubCrawl is an online search website built with the Django web framework backed by a Postgres database and Lucene search index. While the Postgres database stores metadata about papers such as the citation graph, author and venue information, Lucene stores indexes for paper titles, abstracts and fulltext. The PubCrawl interface allows users to submit search terms and relative weights to be assigned to title, abstract, full text searches and PageRank. Figure \ref{interface} shows a screenshot of the interface.

\begin{figure}[t]
\centering
	\includegraphics[width=8in]{images/interface}
	\caption{PubCrawl interface allowing search term and relative weight inputs.}	
	\label{interface}
\end{figure}

\subsection{Scores and Scaling}
PubCrawl combines four scores in all - a fulltext search score, a title search score, an abstract search score and the PageRank score. Since Lucene does't score on a pre-defined scale, we normalize all Lucene scores. Similarly, we normalize PageRanks too. All scores are scaled on a per-query basis instead of a per-corpus basis. This is particularly relevant to PageRank since results for a particular topic may all have low PageRank since the topic is studied by few researchers. By normalizing per-query, we do not penalize topics with a small number of papers.

\subsection{Ranking}
An important challenge faced by PubCrawl is how to effectively combine the four scores above to get an over quality score for a document. We implemented two types of simple ranking functions for this purpose:

\begin{itemize}
	\item 2-factor ranking: As a first step, we only allowed full text and page rank scores to be combined. We used a simple sorting technique for this since every document returned by the search has an associated full text and page rank score.
	\item 4-factor ranking: In order to combine multiple scores, i.e., to combine multiple sorted lists, we used the RankJoin algorithm \cite{DBLP:conf/vldb/IlyasAE03}. Briefly, RankJoin is a top-k algorithm that takes multiple sorted lists and traverses them in a round-robin fashion performing hash joins on already seen tuples. It is able to stop before traversing all tuples because once the max possible score of unseen tuples becomes lesser than the minimum score of the top-k tuples, we are guaranteed to have the required top-k set of tuples. We implemented a slight variation of RankJoin since we don't need joins across all 4 lists.
\end{itemize}

\subsection{Tuning Parameters}

To tune weights for the four factors, we obtained information about seminal papers on 6 topics from experts in the respective areas. We then manually picked weights that gave good results for all 6 topics. The settings we used for the user study were:  title weight (0.1), abstract weight (0), document weight (0.5), page rank (1).

\subsection{PageRank Parameters}
For PubCrawl's implementation of the adapted PageRank scoring function, we used several parameters for variables defined in Section \ref{sec:pagerank}:\\

\begin{itemize}
\item For the score source $E$, we use a uniform value. We minimize the value of $E$ that still results in convergence, to reduce the total amount of freely created score. In our system, we found $E=0.1$ to be appropriate.

\item For initializing node scores, we use a linear relationship with the size of the component containing the node. If the total number of papers (and nodes) is $N$ and paper $p$ is in a component with $X$ nodes, then the initial score for $p$ is set to $\frac{X}{N}$. Thus, larger components start with initially higher scores, and typically converge with higher final scores.

\item For the damping factor of our final scoring function (Equation \ref{eqn:damping}), we use $d=0.85$. This is a reflection of the PageRank study \cite{page99} indicating this value as reasonable, rather than a precisely chosen one.

\end{itemize} 

\section{User Study}

To evaluate the quality of PubCrawl results, we performed a preliminary user study. In this study, we compared the quality of PubCrawl results with those from Google Scholar, Microsoft Academic Search, ACM Digital Library and CiteSeer.

\begin{figure}[t]
\centering
	\includegraphics[width=5in]{images/Relevance1}
	\caption{Relevance of search results grouped as Highly Relevant, Moderately Relevant and Not Relevant.}	
	\label{relevance1}
\end{figure}

\subsection{Methodology}

For 4 topics, we performed searches on Google Scholar, Microsoft Academic Search, ACM Digital Library, CiteSeer and PubCrawl. We then obtained lists of the top 10 ACM publication results from these search engines. We provided these lists to our experts who rated each of the 10 results as Highly Relevant, Moderately Relevant or Not relevant. They also ranked the five lists in order of decreasing quality. Figure \ref{relevance1} shows the distribution of results grouped by ranks assigned by users. We observe that PubCrawl produces a large number of moderately relevant document while it has a low number of highly and not relevant results. While we would ideally like to maximize the former, the low number of non-relevant results is very important since it means that we have low false positive rate. 

\newpage

\begin{figure}[t]
\centering
	\includegraphics[width=5in]{images/Relevance2}
	\caption{Relevance ratio vs. Search Engines.}	
	\label{relevance2}
\end{figure}

\begin{figure}[!ht]
\centering
	\includegraphics[width=5in]{images/Relevance3}
	\caption{Quality Score.}	
	\label{relevance3}
\end{figure}

\newpage

Figures \ref{relevance2} and \ref{relevance3} show two different views of these data. The first plots the relevance ratio across all search engines. Relevance ratio is computed as (\# Highly Relevant + \# Moderately Relevant)/(\#Not Relevant). As we can see, PubCrawl has relevance ratio comparable to Google Scholar and Microsoft Academic Search. CiteSeer, on the other hand, has very poor relevance score since it produces a large number of non-relevant results. ACM Digital Library has the largest number of relevant documents in its results.

In Figure \ref{relevance3}, we show a total quality score we define as follows: Quality Score = \# Highly Relevant Results - \# Non-relevant results + 0.1 * \# Moderately relevant results. We choose this metric to highly rank relevant results, penalize for non-relevant results and factor in moderately relevant results. 

Finally, in Figure \ref{rank1}, we show the average rank assigned to an engine based on the ranks assigned to each engine by our experts. A higher score (scale of 5) indicates a better rank. On average, PubCrawl ranked as high as the other engines except Google Scholar.

\begin{figure}[!h]
\centering
	\includegraphics[width=5in]{images/rank}
	\caption{Rank order of search engines (Higher score indicates better rank; scale of 5).}	
	\label{rank1}
\end{figure}

\subsection{Discussion}
Our preliminary user study only involved 4 topics and hence a larger study must be undertaken to rigorously study the quality of results. We also observed that: (1) quality of results depends heavily on the breadth of the search term (a narrow search term produces better results), (2) ground truth wasn't always clearly known, (3) ground-truth is subjective, (4) since the data only included ACM papers, we had to discard a large number of highly relevant but non-ACM publications. Furthermore, the lack of non-ACM paper citations causes discrepancies in our PageRank algorithm compared to other search engines.

However, our preliminary user study shows that PubCrawl can find relevant documents and effectively tradeoff PageRank and text indexing scores.


\section{Conclusion and Future Work}

In this work, we built a publication search engine based on the PageRank algorithm and the Lucene text index. We adapted PageRank for the specific properties of the citation graph and indexed various paper attributes such as title, abstract and full text. We then explored multiple ways of combining the text indexing scores with PageRank. We observed that it is very difficult to find a scoring function that works well for all search queries. We also realized that combining scores from orthogonal metrics like PageRank and text indexing may not be the best method to obtain an overall ranking. Our user study demonstrates that PubCrawl has a low false positive rate and produces a large number of moderately relevant results. However, it produces a low number of highly relevant results and there is potential to extend this work by finding better ranking functions.

Several extensions of this work are possible. An extremely challenging question we would like to answer is what is the best way to determine appropriate weights in a ranking function. Using labelled data points and curve fitting or SVMs, we are likely to get a better set of results. We would also like to investigate the following heuristics further: it would also be interesting to extend page rank so that it takes into account relationships between papers, i.e. paper A supports paper B, paper A refutes paper B etc. Similarly we would like to use metadata such as paper venues in order to further refine the scoring. We also looked into whether we could classify papers based on the level of expertise a paper expects. This may also be used to boost our search results.


\section{Acknowledgments}
We'd like to thank Samuel Madden, Adam Marcus, and Eugene Wu for their help during this project. They provided insightful observations and guidance throughout.

\bibliography{refs}{}
\bibliographystyle{acm}

\end{document}
