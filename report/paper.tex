\documentclass[12pt]{article}
\setcounter{secnumdepth}{2} 

\title{PubCrawl: A Publication Search Engine}
\author{Frank Li (frankli@mit.edu) \\
Abhishek Sarkar (aksarkar@mit.edu)\\
Manasi Vartak (mvartak@mit.edu)}
\date{May 16, 2012}

\usepackage{graphicx}

\begin{document}
\maketitle


\section{Introduction}

	A primary source of information for computer science researchers are published papers from conference proceedings and journals. However, there are simply too many papers in published in too many venues to keep track of. While experienced researchers are typically versed in the famous and influential papers of their respective fields, they may be less familiar with other areas that might still be needed for their work. Less experienced researchers and students may have even less or no knowledge of the relevant papers of a subject.

	To address this issue, researchers have create paper search engines, such as Google Scholar and CiteseerX. However, the ranking process for these systems is largely privately maintained. In this paper, we investigate the process of ranking in the academic papers setting. An understanding of this is important to create a better search system.
	
	To do this, we designed PubCrawl, a paper search engine, as a testing bed for ranking mechanisms. In building PubCrawl, we used Apache Lucene for text indexing to measure a paper’s relevance to a query, and an adaptation of PageRank on the papers citation graph for weighting a paper’s importance. Currently all papers in the Association of Computing Machinery (ACM) Digital Library database have been processed. A simple web user interface is provided, where users can input queries and weights for what attributes they feel is most important, to tune the result rankings to fit what they feel is most appropriate. The top papers are returned, with hyperlinks to the papers’ ACM homepage.
	
	In this report, we detail the design of PubCrawl and investigate the effects of many of its features. We conduct a user study to compare the performance of our ranking system versus other current systems. 

\section{Related Work}

There are a few paper search engines currently in existence, such as CiteseerX and Google Scholar. Their ranking process is largely maintained privately, but several characteristics of both are described in this section.

CiteseerX's data set is scraped from the web by a crawler, and processed in an automated fashion. This has provided CiteseerX with wide and largely hands-off coverage of papers. However, due to the purely automated approach used and the heterogeneity in data representation on the web, the data collected contains errors and missing information. For example, the crawler has mistaken the wrong string for the title of a number of papers. This leads to some errors in the processing of both the paper data and queries.

Google Scholar also operates with a crawler, and exhibits some of the same dirty data issues as CiteseerX. Additionally, Google Scholar has been shown to be heavily influenced by citation count, which may be inappropriate for some user queries.

\section{ACM Data}
%Information about ACM data


\section{PubCrawl Design}

PubCrawl is designed to rank papers based on relevance and importance. To measure relevance, we use Apache Lucene, a high-performance text search engine library for Java. Lucene is used to provide text indexing, returning the papers most textually relevant to a query. To measure importance, we use a variation of the classic PageRank algorithm on the citation graph of papers. The weighting between the text indexing and PageRank score is an input the users can provide, as discussed in \ref{sec:implementation}. Currently, the ACM dataset has been gathered and applied using PubCrawl. Through the design of the system, we discuss the effect of a number of its features.

\subsection{Lucene Text Indexing}
%Abhishek

\subsection{Adapted PageRank}
\label{sec:pagerank}

The relationship between academic papers draws a strong analogy with that between web pages. Just as pages on the web embed links to other pages, papers cite other papers. This forms a directed graph of paper nodes and citation edges, where an edge from one paper $A$ to another paper $B$ indicates that $A$ cited $B$.

To exploit the similarity between the two environments, we use a variation of the PageRank algorithm. PageRank is an algorithm used by the Google search engine that provides a weighted score for web pages based on the number of external pages that cite a particular web page, and the PageRank of those external pages. We use a similar framework for academic papers, where a paper's score is based on the number of other papers citing it, and the score of those papers. The most simple form of this captured by the following equation:\\
\begin{equation}
\label{eqn:basic}
S(p)= \Sigma_{c \in C} \frac{S(c)}{D(c)}
\end{equation}
where $p$ is the paper being scored, $C$ is the set of all papers that cite $p$, $S$ is the scoring function, and $D$ is the degree function (indicating the number of papers a certain paper cites).

The score of all nodes can be calculated in an iterative manner. All nodes can be initialized to a default score, such as 1. Then, in each round, Equation \ref{eqn:basic} can be applied to all nodes. This process is repeated until node scores converge. However, this scoring function is further complicated by a couple of features of academic paper relations: cycles and components. 

Citation cycles provide one source of complication. In citation cycles, a paper's chain of citations returns to the paper itself. A number of these cycles exist in the data set. In general, these cycles act as a score sink, shifting the score of external nodes into the cycle round after round. Inside the cycle, the score rotates among nodes, preventing convergence. To reduce the impact of these cycles, a score source is provided for node each round, as also done in PageRank. To provide convergence, the total sum of all node scores must remain constant, so node scores must be normalized per round. This results in an updated scoring function of:\\
\begin{equation}
\label{eqn:cycle}
S(p)= n(\Sigma_{c \in C} \frac{S(c)}{D(c)}+E(p))
\end{equation}
where $E$ is a source of score, $n$ is the normalization factor, and all other variables remain as defined.



\begin{figure}[t]
\centering
	\includegraphics[width=5in]{images/component-graph}
	\caption{Sizes of components in the ACM data set and the number of each size's occurrence.}	
	\label{comp-size}
\end{figure}

Components in the citation graph are the other source of complication. Papers in the same component are either both part of the same citation chain or share a common ancestor or descendant in their respective chain. The ACM data exhibits 45,571 total components, with one large component of 176,285 papers, and a large number of smaller components ranging in size from one to ten papers, as displayed in Figure \ref{comp-size}. Small components are papers that don't cite many or any other papers in the data set and aren't heavily cited. This implies that nodes in small components should not be highly ranked. 

To adjust for this, nodes are categorized into the components they belong in. Instead of initializing node scores to be a constant value such as 1, they are instead initialized to a score linear to the size of the component containing the nodes. Intuitively, this means that papers in larger components will receive higher scores, as desired.

A final variation on the basic scoring function is the incorporation of a damping factor $d$. A user following a citation chain will eventually stop, possibly at the end of the chain. Intuitively, the farther down a chain a user traverses, the less likely the papers will relevant and important to the original query, and the more likely the user will stop. The damping factor models this behaviour by reducing the change produced by the scoring function as the number of rounds increases. Adapted from PageRank, this results in the final version of our scoring function:\\
\begin{equation}
\label{eqn:damping}
S(p)= n(\frac{1-d}{N}+d(\Sigma_{c \in C} \frac{S(c)}{D(c)}+E(p)))
\end{equation}
where $d$ is the damping factor, $N$ is the total number of papers/nodes in the network, and all other variables remain as defined.

\section{Implementation}
\label{sec:implementation}
\subsection{Overview}
PubCrawl is an online search website built with the Django web framework backed by a Postgres database and Lucene search index. While the Postgres database stores metadata about papers such as the citation graph, author and venue information, Lucene stores indexes for paper titles, abstracts and fulltext. The PubCrawl interface allows users to submit search terms and relative weights to be assigned to title, abstract, full text searches and pagerank. Figure \ref{interface} shows a screenshot of the interface.

\begin{figure}[t]
\centering
	\includegraphics[width=8in]{images/interface}
	\caption{PubCrawl interface allowing search term and relative weight inputs.}	
	\label{interface}
\end{figure}

\subsection{Scores and Scaling}
PubCrawl combines four scores in all - a fulltext search score, a title search score, an abstract search score and the PageRank score. Since Lucene does't score on a pre-defined scale, we normalize all Lucene. Similarly, we normalize PageRanks. We normalize all scores on a per-query basis. This is particularly relevant to PageRank since results for a particular topic may all have low PageRank since the topic is studied by few researchers. By normalizing per-query, we do not penalize topics with a small number of papers.

\subsection{Ranking}
An important challenge faced by PubCrawl is how to effectively combine the four scores above to get an over quality score for a document. We implemented two types of simple ranking functions for this purpose:

\begin{itemize}
	\item 2-factor ranking: As a first step, we only allowed full text and page rank scores to be combined. We used a simple sorting technique for this since every document returned by the search has an associated full text and page rank score.
	\item 4-factor ranking: In order to combine multiple scores, i.e., to combine multiple sorted lists, we used the RankJoin algorithm. Briefly, RankJoin is a top-k algorithm that takes multiple sorted lists and traverses them in a round-robin fashion performing hash joins on already seen tuples. It is able to stop before traversing all tuples because once the max possible score of unseen tuples becomes lesser than the minimum score of the top-k tuples, we are guaranteed to have the required top-k set of tuples. We implemented a slight variation of RankJoin since we don't need joins across all 4 lists.
\end{itemize}

\subsection{Tuning Parameters}

To tune weights for the four factors, we obtained information about seminal papers on 6 topics from experts in the respective areas. We then manually picked weights that gave good results for all 6 topics.

\subsection{PageRank Parameters}
For PubCrawl's implementation of the adapted PageRank scoring function, we used several parameters for variables defined in Section \ref{sec:pagerank}:\\

\begin{itemize}
\item For the score source $E$, we use a uniform value. We minimize the value of $E$ that still results in convergence, to reduce the total amount of freely created score. In our system, we found $E=0.1$ to be appropriate.

\item For initializing node scores, we use a linear relationship with the size of the component containing the node. If the total number of papers (and nodes) is $N$ and paper $p$ is in a component with $X$ nodes, then the initial score for $p$ is set to $\frac{X}{N}$. Thus, larger components start with initially higher scores, and typically converge with higher final scores.

\item For the damping factor of our final scoring function (Equation \ref{eqn:damping}), we use $d=0.85$. This is a reflection of the PageRank study indicating this value as reasonable, rather than a precisely chosen one.

\end{itemize} 

\section{User Study}

To evaluate the quality of PubCrawl results, we performed a preliminary user study. In this study, we compared the quality of PubCrawl results with those from Google Scholar, Microsoft Academic Search, ACM Digital Library and CiteSeer.

\subsection{Methodology}

For 4 topics, we performed searches on Google Scholar, Microsoft Academic Search, ACM Digital Library, CiteSeer and PubCrawl. We then obtained lists of the top 10 ACM publication results from these search engines. We provided these lists to our experts who rated each of the 10 results as Highly Relevant, Moderately Relevant or Not relevant. They also ranked the five lists in order of decreasing quality. Figure \ref{relevance1} shows the distribution of results grouped by ranks assigned by users. We observe that PubCrawl produces a large number of moderately relevant document while it has a low number of highly and not relevant results. While we would ideally like to maximize the former, the low number of non-relevant results is very important since it means that we have low false positive rate. 

Figures \ref{relevance2} and \ref{reelvance3} show two different views of these data. The first plots the relevance ratio across all search engines. Relevance ratio is computed as (\# Highly Relevant + \# Moderately Relevant)/(\#Not Relevant). As we can see, PubCrawl has relevance ratio comparable to Google Scholar and Microsoft Academic Search. CiteSeer, on the other hand, has very poor relevance score since it produces a large number of non-relevant results. ACM Digital Library has the largest number of relevant documents in its results.

In Figure \ref{relevance3}, we show a total quality score we define as follows: Quality Score = \# Highly Relevant Results - \# Non-relevant results + 0.1 * \# Moderately relevant results. We choose this metric to highly rank relevant results, penalize for non-relevant results and factor in moderately relevant results. 

Finally, in Figure \ref{rank1}, we show the average rank assigned to an engine based on the ranks assigned to each engine by our experts. A higher score (scale of 5) indicates a better rank. On average, PubCrawl ranked as high as the other engines except Google Scholar.

\begin{figure}[t]
\centering
	\includegraphics[width=5in]{images/Relevance1}
	\caption{Relevance of search results grouped as Highly Relevant, Moderately Relevant and Not Relevant.}	
	\label{relevance1}
\end{figure}

\begin{figure}[t]
\centering
	\includegraphics[width=5in]{images/Relevance2}
	\caption{Relevance ratio vs. Search Engines.}	
	\label{relevance2}
\end{figure}

\begin{figure}[t]
\centering
	\includegraphics[width=5in]{images/Relevance3}
	\caption{Quality Score.}	
	\label{relevance3}
\end{figure}

\begin{figure}[t]
\centering
	\includegraphics[width=5in]{images/rank}
	\caption{Rank order of search engines (Higher score indicates better rank; scale of 5).}	
	\label{rank1}
\end{figure}

\subsection{Challenges}
The main challenges with evaluating the user study were (1) Ground truth wasn't always clearly known, (2) Ground-truth is subjective, (3) since the data only included ACM papers, we had to discard a large number of highly relevant but non-ACM publications.

\section{Results}

User study results.

Some time comparisons.

\section{Conclusion and Future Work}

We observed that it is very difficult to find a scoring function that works well for all search queries. We also realized that combining scores from orthogonal metrics like PageRank and text indexing may not be the best method to obtain an overall ranking.

Several extensions of this work are possible. An extremely challenging question we would like to answer is what is the best way to determine appropriate weights in a ranking function. Using labelled data points and curve fitting or SVMs, we are likely to get a better set of results. We would also like to investigate the following heuristics further: it would also be interesting to extend page rank so that it takes into account relationships between papers, i.e. paper A supports paper B, paper A refutes paper B etc. Similarly we would like to use metadata such as paper venues in order to further refine the scoring. We also looked into whether we could classify papers based on the level of expertise a paper expects. This may also be used to boost our search results.


\section{Acknowledgements}
\null

\end{document}
