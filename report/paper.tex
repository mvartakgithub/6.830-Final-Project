\documentclass[12pt]{article}
\setcounter{secnumdepth}{2} 
\title{PubCrawl: A Publication Search Engine}
\author{Frank Li (frankli@mit.edu) \\
Abhishek Sarkar (aksarkar@mit.edu)\\
Manasi Vartak (mvartak@mit.edu)}
\date{May 16, 2012}

\usepackage{graphicx}
\usepackage[letterpaper,margin=1in]{geometry}

\newcommand{\citeseer}{CiteSeer$^\chi$}

\begin{document}
\maketitle

\begin{abstract}
Searching for documents ranked by relevance is a well-studied problem in
information retrieval. In particular, there are frameworks for estimating
relevance as a function of the content of the document and algorithms for
estimating relevance as a function of incoming links. However, further
specialization of these techniques is required to search a corpus of academic
papers. Academic papers are highly structured, lending additional weight to
matches which appear in important sections such as the title or abstract.
Contextual information changes the weight which should be contributed to cited
papers. Here we describe PubCrawl, a search engine built for the ACM Digital
Library. The system allows users to weight the relative importance of matches
in various sections of the documents. We also implement a version of PageRank
over the dataset. We evaluate the system in user studies.
\end{abstract}

\section{Introduction}

A primary source of information for computer science researchers are published
papers from conference proceedings and journals. As the number of papers and
venues increases, the ability to quickly find papers relevant to a given topic
becomes more important. In particular, researchers want search engines which
return important papers.

There are several existing search engines for academic papers, most prominently
Google Scholar, \citeseer, and Microsoft Academic Search. Google Scholar uses a
proprietary algorithm which combines several approaches. However, researchers
have partially reverse engineered it \cite{beel09a, beel09b}. In particular,
the algorithm is biased towards highly cited papers, which should highly rank
important papers in the standard literature. However, the bias
will also make the algorithm miss more recent influential papers. Researchers
also uncovered several weaknesses in Google's automatic processing of documents
which could allow authors to game search rankings \cite{beel10}.

\citeseer\ is primarily an automatic citation indexing system \cite{giles98,
  lawrence99, li06}. The system crawls the web and processes documents which
appear to be academic papers. This approach produces a large corpus of noisy
data, which hurts the performance of its ranking algorithm. Also,
\citeseer\ does not make use of the citation data it processes in its ranking
algorithm.

Microsoft Academic Search is a relatively new search engine designed to compete
with Google Scholar. It too uses a proprietary algorithm which combines several
features, and shows the same weakness to ranking optimization as Scholar.

Here, we present PubCrawl, a system built to test ranking algorithms for
searching academic papers. The system searches over the complete ACM Digital
Library. We use Apache Lucene to index documents and estimate relevance and
PageRank to estimate importance. The system provides a web interface using the
Django web framework backed by a Postgres database. Users can input queries and
relative weights for matches in various sections of the document as well as the
PageRank.
	
\section{PubCrawl Design and Implementation}

\subsection{Overview}

PubCrawl is an online search website for the ACM Digital Library built with the
Django web framework. It is backed by a Postgres database which stores metadata
(authors, venues, and citations) and a Lucene search index. The PubCrawl
interface allows users to submit search terms and assign relative weights to
title, abstract, and full text searches and PageRank. Figure \ref{interface}
shows a screenshot of the interface.

\begin{figure}[bt]
  \begin{center}
    \includegraphics[width=8in]{images/interface}
  \end{center}
  \caption{PubCrawl user interface}
  \label{interface}
\end{figure}

\subsection{ACM Digital Library}

The ACM Digital Library is a corpus of over 223,800 academic papers published
in ACM journals or conference proceedings. The data set is in XML format with
rich metadata including citations. Unlike \citeseer, we do not have to do
sophisticated NLP to build a citation graph, increasing the quality of the
resulting graph.

However, because the data set is limited to ACM publications, the citation
graph is highly disconnected. Although the vast majority of the papers
(176,285) are contained in one component, in total there are 45,571 connected
components. Figure \ref{comp-size} shows the size distribution of these
components is heavily skewed. We take special consideration of component size
in our adaptation of the PageRank algorithm.

\begin{figure}[tb]
  \begin{center}
    \includegraphics[width=5in]{images/component-graph}
  \end{center}
  \caption{Distribution of component sizes in the citation graph}
  \label{comp-size}
\end{figure}

\subsection{Document scores}

We use Apache Lucene \cite{lucene} to index documents and provide scores based
on the content of documents. We build the usual inverted index over the corpus,
filtering out stopwords. The structure of the documents is preserved in the XML
corpus, so we can index titles, subtitles, abstracts, and full text as separate
fields. We also include various metadata to simplify the implementation of the
front-end.

Lucene uses the Boolean model \cite{lancaster73} to filter documents and the
Vector Space Model \cite{salton75} to score documents. Documents and queries
are vectors with each term a dimension and coordinates equal to tf*idf values.
Then, the relevance of a document to a query is related to the angle between
them (the cosine similarity score). Lucene generalizes the cosine similarity
score in several ways. First, documents are allowed to have multiple and
duplicate fields. Second, Lucene allows index-time boosting of documents and
fields as well as query-time boosting of query terms. Finally, Lucene does not
require every term to appear in hits, instead using a coordination factor to
score documents which contain more terms higher. These features can be
exploited to improve the quality of search results.

\subsection{Adapted PageRank}

PageRank is a well-known algorithm for estimating the importance of a web page
as a function of the number of incoming links \cite{brin98, page99}. It is
plausible to extend the algorithm by analogy to academic papers and citations.
We use the iterative algorithm as described but tune the parameters to improve
the quality of the results for the ACM data set.

We set the score source $E$ to a uniform vector with each coordinate equal to
$0.1$. This setting minimizes the PageRank each paper gets for free while still
allowing the algorithm to converge. We set the damping factor $d$ to the
suggested value of $0.85$. This setting reduces the number of iterations needed
to converge by reducing the amount of PageRank contributed in subsequent
iterations.

We found initializing the PageRank of each paper to a constant gave suboptimal
results because the ACM citation graph has a high number of small components.
Papers in these components end with disproportionately high PageRank because
further iterations do not change their initial value very much. Instead, we set
the initial value of a paper's PageRank to the proportion of all papers
contained in its component.

\subsection{Combining document scores and PageRanks}

We compute a final score used for ranking documents as a linear function of
subscores. One issue is that these subscores are not necessarily on the same
scale. In particular, Lucene scores are on an arbitrary scale. To address this
issue, we normalize scores per query.

The PageRank algorithm includes a normalization step at each iteration.
However, if a particular topic is studied by only a few researchers, all the
papers relevant to that topic will have low PageRank. To address this issue, we
renormalize PageRanks per query.

We implemented two ranking functions. The first only uses the full text score
and PageRank. The second uses a rank join--inspired algorithm to combine lists
of hits sorted by each of title, abstract, and full text scores and PageRank.
Briefly, the algorithm maintains the top $k$ hits seen so far. In each
iteration, we pop the maximum element off of the next list in round-robin order
and hash it. When we encounter a duplicate, we update the combined score and
its position in the top $k$. The algorithm terminates when the maximum possible
score of unseen tuples is less than the score of the $k$th hit.

To tune weights for the four factors, we obtained information about important
papers on 6 topics from experts in the respective areas. We then manually
picked weights that gave good results for all 6 topics.

\section{User Study}

To evaluate the quality of PubCrawl results, we performed a preliminary user study. In this study, we compared the quality of PubCrawl results with those from Google Scholar, Microsoft Academic Search, ACM Digital Library and \citeseer.

\subsection{Methodology}

For 3 topics, we performed searches on Google Scholar, Microsoft Academic Search, ACM Digital Library, CiteSeer and PubCrawl. We then obtained lists of the top 10 ACM publication results from these search engines. We provided these lists to our experts. The experts rated each of the 10 results as Highly Relevant, Moderately Relevant and Not relevant. They also ranked the five lists in order of decreasing quality. Table \ref{} shows the results for the 3 test topics.


We find that PubCrawl consistently does as well as CiteSeer and ACM DL. 

\subsection{Challenges}
The main challenges with evaluating the user study were (1) Ground truth wasn't always clearly known, (2) Ground-truth is subjective, (3) since the data only included ACM papers, we had to discard a large number of highly relevant but non-ACM publications.

\section{Results}

User study results.

Some time comparisons.

\section{Conclusion and Future Work}

We observed that it is very difficult to find a scoring function that works well for all search queries. We also realized that combining scores from orthogonal metrics like PageRank and text indexing may not be the best method to obtain an overall ranking.

Several extensions of this work are possible. An extremely challenging question we would like to answer is what is the best way to determine appropriate weights in a ranking function. Using labeled data points and curve fitting or SVMs, we are likely to get a better set of results. We would also like to investigate the following heuristics further: it would also be interesting to extend page rank so that it takes into account relationships between papers, i.e. paper A supports paper B, paper A refutes paper B etc. Similarly we would like to use metadata such as paper venues in order to further refine the scoring. We also looked into whether we could classify papers based on the level of expertise a paper expects. This may also be used to boost our search results.


\section{Acknowledgments}

\bibliography{refs}{}
\bibliographystyle{acm}

\end{document}

\end{document}
