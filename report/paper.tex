\documentclass[12pt]{article}
\setcounter{secnumdepth}{2} 
\title{PubCrawl: A Publication Search Engine}
\author{Frank Li (frankli@mit.edu) \\
Abhishek Sarkar (aksarkar@mit.edu)\\
Manasi Vartak (mvartak@mit.edu)}
\date{May 16, 2012}

\usepackage{graphicx}
\usepackage[letterpaper,margin=1in]{geometry}

\newcommand{\citeseer}{CiteSeer$^\chi$}

\begin{document}
\maketitle

\begin{abstract}
Searching for documents ranked by relevance is a well-studied problem in
information retrieval. In particular, there are frameworks for estimating
relevance as a function of the content of the document and algorithms for
estimating relevance as a function of incoming links. However, further
specialization of these techniques is required to search a corpus of academic
papers. Academic papers are highly structured, lending additional weight to
matches which appear in important sections such as the title or abstract.
Contextual information changes the weight which should be contributed to cited
papers. Here we describe PubCrawl, a search engine built for the ACM Digital
Library. The system allows users to weight the relative importance of matches
in various sections of the documents. We also implement a version of PageRank
over the dataset. We evaluate the system in user studies.
\end{abstract}

\section{Introduction}

A primary source of information for computer science researchers are published
papers from conference proceedings and journals. As the number of papers and
venues increases, the ability to quickly find papers relevant to a given topic
becomes more important. In particular, researchers want search engines which
return important papers.

There are several existing search engines for academic papers, most prominently
Google Scholar, \citeseer, and Microsoft Academic Search. Google Scholar uses a
proprietary algorithm which combines several approaches. However, researchers
have partially reverse engineered it \cite{beel09a, beel09b}. In particular,
the algorithm is biased towards highly cited papers, which should highly rank
important papers in the standard literature. However, the bias
will also make the algorithm miss more recent influential papers. Researchers
also uncovered several weaknesses in Google's automatic processing of documents
which could allow authors to game search rankings \cite{beel10}.

\citeseer\ is primarily an automatic citation indexing system \cite{giles98,
  lawrence99, li06}. The system crawls the web and processes documents which
appear to be academic papers. This approach produces a large corpus of noisy
data, which hurts the performance of its ranking algorithm. Also,
\citeseer\ does not make use of the citation data it processes in its ranking
algorithm.

Microsoft Academic Search is a relatively new search engine designed to compete
with Google Scholar. It too uses a proprietary algorithm which combines several
features, and shows the same weakness to ranking optimization as Scholar.

Here, we present PubCrawl, a system built to test ranking algorithms for
searching academic papers. The system searches over the complete ACM Digital
Library. We use Apache Lucene to index documents and estimate relevance and
PageRank to estimate importance. The system provides a web interface using the
Django web framework backed by a Postgres database. Users can input queries and
relative weights for matches in various sections of the document as well as the
PageRank.
	
\section{PubCrawl Design}

\section{ACM Digital Library}

The ACM Digital Library is a corpus of over 223,800 academic papers published
in ACM journals or conference proceedings. The data set is in XML format with
rich metadata including citations. Unlike \citeseer, we do not have to do
sophisticated NLP to build a citation graph, increasing the quality of the
resulting graph.

However, because the data set is limited to ACM publications, the citation
graph is highly disconnected. Although the vast majority of the papers
(176,285) are contained in one component, in total there are 45,571 connected
components. Figure \ref{comp-size} shows the size distribution of these
components is heavily skewed. We take special consideration of component size
in our adaptation of the PageRank algorithm.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=5in]{images/component-graph}
  \end{center}
  \caption{Distribution of component sizes in the citation graph}
  \label{comp-size}
\end{figure}

\subsection{Document scores}

We use Apache Lucene \cite{lucene} to index documents and provide scores based
on the content of documents. We build the usual inverted index over the corpus,
filtering out stopwords. The structure of the documents is preserved in the XML
corpus, so we can index titles, subtitles, abstracts, and full text as separate
fields. We also include various metadata to simplify the implementation of the
front-end.

Lucene uses the Boolean model \cite{lancaster73} to filter documents and the
Vector Space Model \cite{salton75} to score documents. Documents and queries
are vectors with each term a dimension and coordinates equal to tf*idf values.
Then, the relevance of a document to a query is related to the angle between
them (the cosine similarity score). Lucene generalizes the cosine similarity
score in several ways. First, documents are allowed to have multiple and
duplicate fields. Second, Lucene allows index-time boosting of documents and
fields as well as query-time boosting of query terms. Finally, Lucene does not
require every term to appear in hits, instead using a coordination factor to
score documents which contain more terms higher. These features can be
exploited to improve the quality of search results.

\subsection{Adapted PageRank}
\label{sec:pagerank}

PageRank is a well-known algorithm for estimating the importance of a web page
as a function of the number of incoming links \cite{brin98, page99}. It is
plausible to extend the algorithm by analogy to academic papers and citations.
However, applying PageRank directly to the ACM citation graph gives suboptimal
results due to the high number of small components. Papers in these components
end with disproportionately high PageRank because further iterations do not
change their initial value very much. Instead of a constant, we set the initial
value of a paper's PageRank to the proportion of all papers contained in its
component.

\section{Implementation}
\label{sec:implementation}
\subsection{Overview}
PubCrawl is an online search website built with the Django web framework backed by a Postgres database and Lucene search index. While the Postgres database stores metadata about papers such as the citation graph, author and venue information, Lucene stores indexes for paper titles, abstracts and fulltext. The PubCrawl interface allows users to submit search terms and relative weights to be assigned to title, abstract, full text searches and PageRank. Figure \ref{interface} shows a screenshot of the interface.

\begin{figure}[t]
\centering
	\includegraphics[width=8in]{images/interface}
	\caption{PubCrawl interface allowing search term and relative weight inputs.}	
	\label{interface}
\end{figure}

\subsection{Scores and Scaling}
PubCrawl combines four scores in all - a fulltext search score, a title search score, an abstract search score and the PageRank score. Since Lucene does't score on a pre-defined scale, we normalize all Lucene. Similarly, we normalize PageRanks. We normalize all scores on a per-query basis. This is particularly relevant to PageRank since results for a particular topic may all have low PageRank since the topic is studied by few researchers. By normalizing per-query, we do not penalize topics with a small number of papers.

\subsection{Ranking}
An important challenge faced by PubCrawl is how to effectively combine the four scores above to get an over quality score for a document. We implemented two types of simple ranking functions for this purpose:

\begin{itemize}
	\item 2-factor ranking: As a first step, we only allowed full text and page rank scores to be combined. We used a simple sorting technique for this since every document returned by the search has an associated full text and page rank score.
	\item 4-factor ranking: In order to combine multiple scores, i.e., to combine multiple sorted lists, we used the RankJoin algorithm. Briefly, RankJoin is a top-k algorithm that takes multiple sorted lists and traverses them in a round-robin fashion performing hash joins on already seen tuples. It is able to stop before traversing all tuples because once the max possible score of unseen tuples becomes lesser than the minimum score of the top-k tuples, we are guaranteed to have the required top-k set of tuples. We implemented a slight variation of RankJoin since we don't need joins across all 4 lists.
\end{itemize}

\subsection{Tuning Parameters}

To tune weights for the four factors, we obtained information about seminal papers on 6 topics from experts in the respective areas. We then manually picked weights that gave good results for all 6 topics.

\subsection{PageRank Parameters}
For PubCrawl's implementation of the adapted PageRank scoring function, we used several parameters for variables defined in Section \ref{sec:pagerank}:\\

\begin{itemize}
\item For the score source $E$, we use a uniform value. We minimize the value
  of $E$ that still results in convergence, to reduce the total amount of
  freely created score. In our system, we found $E=0.1$ to be appropriate.

\item For initializing node scores, we use a linear relationship with the size
  of the component containing the node. If the total number of papers (and
  nodes) is $N$ and paper $p$ is in a component with $X$ nodes, then the
  initial score for $p$ is set to $\frac{X}{N}$. Thus, larger components start
  with initially higher scores, and typically converge with higher final
  scores.

\item For the damping factor of our final scoring function (Equation
  \ref{eqn:damping}), we use $d=0.85$. This is a reflection of the PageRank
  study indicating this value as reasonable, rather than a precisely chosen
  one.

\end{itemize} 

\section{User Study}

To evaluate the quality of PubCrawl results, we performed a preliminary user study. In this study, we compared the quality of PubCrawl results with those from Google Scholar, Microsoft Academic Search, ACM Digital Library and \citeseer.

\subsection{Methodology}

For 3 topics, we performed searches on Google Scholar, Microsoft Academic Search, ACM Digital Library, CiteSeer and PubCrawl. We then obtained lists of the top 10 ACM publication results from these search engines. We provided these lists to our experts. The experts rated each of the 10 results as Highly Relevant, Moderately Relevant and Not relevant. They also ranked the five lists in order of decreasing quality. Table \ref{} shows the results for the 3 test topics.


We find that PubCrawl consistently does as well as CiteSeer and ACM DL. 

\subsection{Challenges}
The main challenges with evaluating the user study were (1) Ground truth wasn't always clearly known, (2) Ground-truth is subjective, (3) since the data only included ACM papers, we had to discard a large number of highly relevant but non-ACM publications.

\section{Results}

User study results.

Some time comparisons.

\section{Conclusion and Future Work}

We observed that it is very difficult to find a scoring function that works well for all search queries. We also realized that combining scores from orthogonal metrics like PageRank and text indexing may not be the best method to obtain an overall ranking.

Several extensions of this work are possible. An extremely challenging question we would like to answer is what is the best way to determine appropriate weights in a ranking function. Using labeled data points and curve fitting or SVMs, we are likely to get a better set of results. We would also like to investigate the following heuristics further: it would also be interesting to extend page rank so that it takes into account relationships between papers, i.e. paper A supports paper B, paper A refutes paper B etc. Similarly we would like to use metadata such as paper venues in order to further refine the scoring. We also looked into whether we could classify papers based on the level of expertise a paper expects. This may also be used to boost our search results.


\section{Acknowledgments}

\bibliography{refs}{}
\bibliographystyle{acm}

\end{document}

\end{document}
